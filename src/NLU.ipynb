{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQCewY5joZMS",
    "colab_type": "text"
   },
   "source": [
    "- Can we calculate perplexity with a base of exp? much easier from the loss directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "5cgxwK_cOpLe",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)  # suppress some deprecation warnings\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "IBYiR6-kOt76",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# Tensorboard in Colab environment.\n",
    "!pip install tensorboardcolab\n",
    "from tensorboardcolab import *\n",
    "tbc = TensorBoardColab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "_eN67XwKROqu",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# upload rest .py files to colab\n",
    "# model.py, data_processing.py, utils.py\n",
    "!rm *.py\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "bJryIDLORryW",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# used to be able to rerun this cell\n",
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "log_dir = \"/tmp/tensorflow/logs\"\n",
    "\n",
    "# needed to avoid errors in this version of tensorflow\n",
    "tf.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "tf.flags.DEFINE_string(\"log_dir\", log_dir, \"Summaries log directory\")\n",
    "tf.flags.DEFINE_integer(\"embedding_size\", 100, \"embedding size (default 100)\")\n",
    "tf.flags.DEFINE_integer(\"hidden_state_size\", 512, \"hidden state size (default 512)\")\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"max_vocabulary_size\", 20000, \"Maximum vocabulary size (default: 20000)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 10, \"Number of training epochs (default: 10)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 500, \"Evaluate model on dev set after this many steps (default: 1000)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 5000, \"Save model after this many steps (default: 5000)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 50, \"Number of checkpoints to store (default: 50)\")\n",
    "tf.flags.DEFINE_integer(\"sentence_length\", 30, \"Maximum length of a sentence (default 30)\")\n",
    "tf.flags.DEFINE_boolean(\"load_embeddings\", False, \"Whether to use pretrained embeddings or not (default False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oHPeGT5bc-6E",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272.0
    },
    "outputId": "b89bbf7a-1175-4654-dbbf-705aef9309cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Command-line Arguments:\n",
      "LOG_DIR               : /tmp/tensorflow/logs\n",
      "EMBEDDING_SIZE        : 100\n",
      "HIDDEN_STATE_SIZE     : 512\n",
      "BATCH_SIZE            : 64\n",
      "MAX_VOCABULARY_SIZE   : 20000\n",
      "NUM_EPOCHS            : 10\n",
      "EVALUATE_EVERY        : 500\n",
      "CHECKPOINT_EVERY      : 5000\n",
      "NUM_CHECKPOINTS       : 50\n",
      "SENTENCE_LENGTH       : 30\n",
      "LOAD_EMBEDDINGS       : False\n",
      " \n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "print(\"\\nCommand-line Arguments:\")\n",
    "for key in FLAGS.flag_values_dict():\n",
    "    if key == 'f':\n",
    "        continue\n",
    "    print(\"{:<22}: {}\".format(key.upper(), FLAGS[key].value))\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oh66j22_OvFL",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72.0
    },
    "outputId": "5f17ca95-4c9e-4b56-91c3-01886cc0a661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size\t\t\t\t20000\n",
      "Number of train sentences is \t\t1969833\n",
      "Number of validation sentences is \t9846\n"
     ]
    }
   ],
   "source": [
    "from data_processing import DataProcessing\n",
    "\n",
    "dataProcessing = DataProcessing(FLAGS.sentence_length, FLAGS.max_vocabulary_size)\n",
    "dataProcessing.preprocess_dataset('/gdrive/My Drive/Colab Notebooks/NLU/data/', 'sentences.train', 'sentences.eval')\n",
    "\n",
    "print(f'Vocabulary size\\t\\t\\t\\t{len(dataProcessing.vocab)}')\n",
    "print(f'Number of train sentences is \\t\\t{len(dataProcessing.train_corpus)}')\n",
    "print(f'Number of validation sentences is \\t{len(dataProcessing.validation_corpus)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "9La0CS8nSKpQ",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108.0
    },
    "outputId": "06f91524-4de0-447c-db61-7cd4a4f4eb89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation average perplexity per across sentences is 271.690 at step 500\n",
      "Evaluation average perplexity per across sentences is 199.209 at step 1000\n",
      "Evaluation average perplexity per across sentences is 170.258 at step 1500\n",
      "Evaluation average perplexity per across sentences is 151.728 at step 2000\n",
      "Evaluation average perplexity per across sentences is 141.059 at step 2500\n"
     ]
    }
   ],
   "source": [
    "from model import LSTMCell\n",
    "from utils import train_batch, dev_step\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as session:\n",
    "        # Create a variable to contain a counter for the global training step.\n",
    "        global_step = tf.Variable(1, name='global_step', trainable=False)\n",
    "\n",
    "        lstm = LSTMCell(FLAGS.embedding_size, FLAGS.hidden_state_size, FLAGS.sentence_length, \n",
    "                        FLAGS.max_vocabulary_size, pad_symbol=dataProcessing.vocab['<pad>'])\n",
    "\n",
    "        ####\n",
    "        ## Set optimizer and crop all gradients to values [-5, 5]\n",
    "        #### \n",
    "        with tf.name_scope('train'):\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "            gvs = optimizer.compute_gradients(lstm.loss)\n",
    "            capped_gvs = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gvs]\n",
    "            train_step = optimizer.apply_gradients(capped_gvs, global_step=global_step)\n",
    "\n",
    "\n",
    "        # Tensorboard\n",
    "        train_writer = tbc.get_deep_writers(\"single_layer_model/train\")\n",
    "        train_writer.add_graph(session.graph)\n",
    "        valid_writer = tbc.get_deep_writers(\"single_layer_model/valid\")\n",
    "        valid_writer.add_graph(session.graph)\n",
    "\n",
    "\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        summaries_merged = tf.summary.merge(lstm.summaries)\n",
    "        \n",
    "        \n",
    "        ####\n",
    "        ## Create checkpoint directory\n",
    "        #### \n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(FLAGS.log_dir, \"runs\", timestamp))\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "            \n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "\n",
    "        ####\n",
    "        ## Start training for the specified epochs\n",
    "        #### \n",
    "        for epoch in range(FLAGS.num_epochs):\n",
    "            for sentences_batch in dataProcessing.get_batches(dataProcessing.data_train, batch_size=FLAGS.batch_size):\n",
    "                \n",
    "                # run a single step\n",
    "                train_batch(sentences_batch, lstm, train_step, global_step, session, train_writer, summaries_merged)\n",
    "                \n",
    "                current_step = tf.train.global_step(session, global_step)\n",
    "                \n",
    "                if current_step % FLAGS.evaluate_every == 0:\n",
    "#                 if current_step % 1 == 0:\n",
    "                    dev_step(dataProcessing.get_batches(dataProcessing.data_validation, batch_size=FLAGS.batch_size), \n",
    "                             lstm, global_step, session, valid_writer, summaries_merged)\n",
    "                    \n",
    "                if current_step % FLAGS.checkpoint_every == 0:\n",
    "                    path = saver.save(session, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\".format(path))\n",
    "\n",
    "            print('Done with epoch', epoch + 1)\n",
    "\n",
    "\n",
    "        train_writer.flush()\n",
    "        valid_writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "akIZ5bI6SKtt",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "q-D9Djluu6B7",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "nlu.ipynb",
   "version": "0.3.2",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
