{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQCewY5joZMS",
    "colab_type": "text"
   },
   "source": [
    "- Can we calculate perplexity with a base of exp? much easier from the loss directly\n",
    "- get batches in utils instead?\n",
    "- down project activation (nope)\n",
    "- what is the length of the continuation sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "5cgxwK_cOpLe",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)  # suppress some deprecation warnings\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "IBYiR6-kOt76",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# Tensorboard in Colab environment.\n",
    "!pip install tensorboardcolab\n",
    "from tensorboardcolab import *\n",
    "tbc = TensorBoardColab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "_eN67XwKROqu",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# upload rest .py files to colab\n",
    "# model.py, data_processing.py, utils.py\n",
    "!rm *.py\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "bJryIDLORryW",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# used to be able to rerun this cell\n",
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "log_dir = \"/tmp/tensorflow/logs\"\n",
    "\n",
    "# needed to avoid errors in this version of tensorflow\n",
    "tf.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "tf.flags.DEFINE_string(\"log_dir\", log_dir, \"Summaries log directory\")\n",
    "tf.flags.DEFINE_integer(\"embedding_size\", 100, \"embedding size (default 100)\")\n",
    "tf.flags.DEFINE_integer(\"hidden_state_size\", 512, \"hidden state size (default 512)\")\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"max_vocabulary_size\", 20000, \"Maximum vocabulary size (default: 20000)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 10, \"Number of training epochs (default: 10)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 500, \"Evaluate model on dev set after this many steps (default: 1000)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 5000, \"Save model after this many steps (default: 5000)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 50, \"Number of checkpoints to store (default: 50)\")\n",
    "tf.flags.DEFINE_integer(\"sentence_length\", 30, \"Maximum length of a sentence (default 30)\")\n",
    "tf.flags.DEFINE_boolean(\"load_embeddings\", False, \"Whether to use pretrained embeddings or not (default False)\")\n",
    "tf.flags.DEFINE_integer(\"down_project_size\", None, \n",
    "                        \"Down projection size. Should be used with a hidden_state_size of 1024 (default None)\")\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "assert(FLAGS.down_project_size == 512 and FLAGS.hidden_state_size == 1024 or \n",
    "       FLAGS.down_project_size is None and FLAGS.hidden_state_size == 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oHPeGT5bc-6E",
    "colab_type": "code",
    "outputId": "485b50fa-c754-4735-da6c-692e40b79ea0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Command-line Arguments:\n",
      "LOG_DIR               : /tmp/tensorflow/logs\n",
      "EMBEDDING_SIZE        : 100\n",
      "HIDDEN_STATE_SIZE     : 512\n",
      "BATCH_SIZE            : 64\n",
      "MAX_VOCABULARY_SIZE   : 20000\n",
      "NUM_EPOCHS            : 10\n",
      "EVALUATE_EVERY        : 500\n",
      "CHECKPOINT_EVERY      : 5000\n",
      "NUM_CHECKPOINTS       : 50\n",
      "SENTENCE_LENGTH       : 30\n",
      "LOAD_EMBEDDINGS       : False\n",
      "DOWN_PROJECT_SIZE     : None\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCommand-line Arguments:\")\n",
    "for key in FLAGS.flag_values_dict():\n",
    "    if key == 'f':\n",
    "        continue\n",
    "    print(\"{:<22}: {}\".format(key.upper(), FLAGS[key].value))\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oh66j22_OvFL",
    "colab_type": "code",
    "outputId": "b6497b06-7822-4f1a-c50e-8d5331072e7a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size\t\t\t\t20000\n",
      "Number of train sentences is \t\t1969833\n",
      "Number of validation sentences is \t9846\n",
      "Number of continuation sentences is \t10000\n"
     ]
    }
   ],
   "source": [
    "from data_processing import DataProcessing\n",
    "\n",
    "dataProcessing = DataProcessing(FLAGS.sentence_length, FLAGS.max_vocabulary_size)\n",
    "dataProcessing.preprocess_dataset('/gdrive/My Drive/Colab Notebooks/NLU/data/', 'sentences.train', 'sentences.eval', 'sentences.continuation')\n",
    "\n",
    "print(f'Vocabulary size\\t\\t\\t\\t{len(dataProcessing.vocab)}')\n",
    "print(f'Number of train sentences is \\t\\t{len(dataProcessing.train_corpus)}')\n",
    "print(f'Number of validation sentences is \\t{len(dataProcessing.validation_corpus)}')\n",
    "print(f'Number of continuation sentences is \\t{len(dataProcessing.continuation_corpus)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "FyMbgqniE-MQ",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from model import LSTMCell\n",
    "from utils import train_batch, dev_step, load_embedding, continue_sentence\n",
    "from data_processing import get_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "9La0CS8nSKpQ",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54.0
    },
    "outputId": "830b3778-b3f2-4f26-e136-725b9b0dfcc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation average perplexity per across sentences is 275.453 at step 500\n",
      "Generated sentence # when he was . # at step 500\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as session:\n",
    "        # Create a variable to contain a counter for the global training step.\n",
    "        global_step = tf.Variable(1, name='global_step', trainable=False)\n",
    "\n",
    "        lstm = LSTMCell(FLAGS.embedding_size, FLAGS.hidden_state_size, FLAGS.sentence_length, \n",
    "                        FLAGS.max_vocabulary_size, down_project_size=FLAGS.down_project_size, \n",
    "                        pad_symbol=dataProcessing.vocab['<pad>'])\n",
    "\n",
    "        if FLAGS.load_embeddings:\n",
    "            load_embedding(session, dataProcessing.vocab, lstm.input_embeddings, \n",
    "                           '/gdrive/My Drive/Colab Notebooks/NLU/data/wordembeddings-dim100.word2vec',\n",
    "                           FLAGS.embedding_size, len(dataProcessing.vocab))\n",
    "            \n",
    "        ####\n",
    "        ## Set optimizer and crop all gradients to values [-5, 5]\n",
    "        #### \n",
    "        with tf.name_scope('train'):\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "            gvs = optimizer.compute_gradients(lstm.loss)\n",
    "            capped_gvs = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gvs]\n",
    "            train_step = optimizer.apply_gradients(capped_gvs, global_step=global_step)\n",
    "\n",
    "\n",
    "        # Tensorboard\n",
    "        train_writer = tbc.get_deep_writers(\"single_layer_model/train\")\n",
    "        train_writer.add_graph(session.graph)\n",
    "        valid_writer = tbc.get_deep_writers(\"single_layer_model/valid\")\n",
    "        valid_writer.add_graph(session.graph)\n",
    "\n",
    "\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        summaries_merged = tf.summary.merge(lstm.summaries)\n",
    "        \n",
    "        \n",
    "        ####\n",
    "        ## Create checkpoint directory\n",
    "        #### \n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(FLAGS.log_dir, \"runs\", timestamp))\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "            \n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "\n",
    "        ####\n",
    "        ## Start training for the specified epochs\n",
    "        #### \n",
    "        for epoch in range(FLAGS.num_epochs):\n",
    "            for sentences_batch in get_batches(dataProcessing.train_corpus, batch_size=FLAGS.batch_size):\n",
    "                \n",
    "                # run a single step\n",
    "                train_batch(sentences_batch, lstm, train_step, global_step, session, summaries_merged, train_writer)\n",
    "                \n",
    "                current_step = tf.train.global_step(session, global_step)\n",
    "                \n",
    "                if current_step % FLAGS.evaluate_every == 0:\n",
    "                    dev_step(get_batches(dataProcessing.validation_corpus, batch_size=FLAGS.batch_size), \n",
    "                             lstm, global_step, session, valid_writer, summaries_merged)\n",
    "                    \n",
    "                    generated_sentence = continue_sentence(dataProcessing.continuation_corpus[5], session, lstm, \n",
    "                                                           dataProcessing.inverse_vocab, dataProcessing.vocab['<eos>'])\n",
    "                    print('Generated sentence #', generated_sentence, '# at step', current_step)\n",
    "                \n",
    "                    \n",
    "                if current_step % FLAGS.checkpoint_every == 0:\n",
    "                    path = saver.save(session, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\".format(path))\n",
    "\n",
    "            print('Done with epoch', epoch + 1)\n",
    "\n",
    "\n",
    "        train_writer.flush()\n",
    "        valid_writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Aycxfm8FTCsv",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# How to restore session\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as session:\n",
    "        global_step = tf.Variable(1, name='global_step', trainable=False)\n",
    "        lstm = LSTMCell(FLAGS.embedding_size, FLAGS.hidden_state_size, FLAGS.sentence_length, \n",
    "                        FLAGS.max_vocabulary_size, down_project_size=FLAGS.down_project_size, \n",
    "                        pad_symbol=dataProcessing.vocab['<pad>'])\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(session, \"/tmp/tensorflow/logs/runs/1554549194/checkpoints/model-5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "akIZ5bI6SKtt",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!ls /tmp/tensorflow/logs/runs/1554549194/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "q-D9Djluu6B7",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# files.download('/tmp/tensorflow/logs/runs/1554499773/checkpoints/model-30000.data-00000-of-00001')\n",
    "# files.download('/tmp/tensorflow/logs/runs/1554499773/checkpoints/model-30000.index')\n",
    "# files.download('/tmp/tensorflow/logs/runs/1554499773/checkpoints/model-30000.meta')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "nlu.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
