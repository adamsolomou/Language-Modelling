# -*- coding: utf-8 -*-
"""lstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14jqDQ4AH0yCcqaMijD6_eLOp8euHPU52
"""

from __future__ import print_function, division
import numpy as np
import tensorflow as tf
import numpy as np
import pandas as pd
import time
import math
from collections import Counter

# drive.mount('/content/drive')

device_name = tf.test.gpu_device_name()

if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
  
print('Found GPU at: {}'.format(device_name))

path = "/cluster/home/saioanni/NLU/data/sentences.train"
eval_path = "/cluster/home/saioanni/NLU/data/sentences.eval"
test_path = "/cluster/home/saioanni/NLU/data/sentences_test.txt"
cont_path = "/cluster/home/saioanni/NLU/data/sentences.continuation"


# we need these initilizations here 
BOS = 0
UNK = 1
PAD = 2
EOS = 3

def get_sentences_from_file(path=None, num_sentences=None, max_length=30, add_eos=True):
  '''
  Description
  Read sentences from file. Read the number of sentences indicated by num_sentences. 
  Each sentence is represented by a list of words.
  The sentences are initialized as a list which consists of <pad> chars.
  If read lines returns '' then there are no more sentences to read, thus we should
  stop reading the file.
  Sentences that exceed max_length do not count.
  Otherwise we set sentence[0] as <bos> and add the words and increase their corresponding counter
  After having read all the words in the sentence we add '<eos>'.
  Args:
  path: path of file
  num_sentences: maximum number of sentences to read from the file. Notice that we count only the
  sentences that do not exceed the max_length limit.
  max_length: Maximum length of sentence after the parsing
  Return: sentences [[char]]
  '''
  
  # If no maximum number of sentences is set then read the whole file
  loop_forever = (num_sentences == None)
  if loop_forever:
    num_lines = 0
  else:
    num_lines = num_sentences
  
  file = open(path,'r')

  lines = []

  sentences = [] # Each sentecse is rep

  loop_forever = (num_sentences is None)
  
  row = 0
  
  while loop_forever or num_lines > 0:
      sentence = max_length*['<pad>']
      line = file.readline().strip()

      if line == '': break

      words = line.split()
      sentence_length = len(words)

      if len(words) > max_length - 2:
          continue

      sentence[0] = '<bos>'

      for i,w in enumerate(words):
          sentence[i+1] = w

      if add_eos:
        sentence[sentence_length] = '<eos>'

      sentences.append(sentence)
      
      num_lines -= 1
      
      
  return sentences

def build_vocabulary(sentences, 
                     vocab_size=20000, 
                     extra_words=['<bos>','<unk>','<pad>','<eos>']):
  '''
  Description:
  Count appereance of each word in a sentence.
  At the end ignore the extra words.
  Return a list corresponding to the vocabulary and don't forget to put the extra words
  in a specific position.
  Args:
  sentences: a list of list of words
  vocab_size: the size of vocabulary (includes extra_words)
  extra_words: extra words to use
  Return:
  [char]: just a list of the words in the vocabulary. Notice that we should 
  keep a certain position for the extra words.
  
  '''
  
  vocabulary = Counter()
  
  for s in sentences:
    for w in s:
      vocabulary[w] += 1 
      
  for ew in extra_words:
    del vocabulary[ew]
    
  vocabulary,_ = zip(*list(vocabulary.most_common(vocab_size - len(extra_words))))
  vocabulary = extra_words + list(vocabulary)
  assert(vocabulary[PAD] == '<pad>')
  assert(vocabulary[UNK] == '<unk>')
  
  return vocabulary

class Encoder:

  def __init__(self, vocabulary):

    self.vocabulary = vocabulary
    self.vocab2num = dict([(w,i) for i,w in enumerate(self.vocabulary)])
    self.num2vocab = dict([(i,w) for i,w in enumerate(self.vocabulary)])
    assert(self.num2vocab[self.vocab2num['<pad>']] == '<pad>')

  def encode_word(self, word):
    return self.vocab2num.get(word,UNK)
  
  def decode_word(self, word):
    return self.num2vocab[word]
    
  def encode_sentence(self, sentence):
    '''
    Returns the encoded sentence; Each word is converted to number.
    '''
    return [self.vocab2num.get(w,UNK) for w in sentence]
  
  def decode_sentence(self, sentence):
    '''
    Returns the decoded sentence; The reversed procedure than the above.
    Notice that the reverse procedure may not be able to restore the sentence
    completely since <unk> corresponds to more than one word
    '''
    return [self.num2vocab[n] for n in sentence]
  
  def encode_dataset(self, dataset):
    '''
    Encode the whole data set.
    args: dataset (list of sentences)
    returns: np.array of numbers 
    '''
    encoded_dataset = [self.encode_sentence(s) for s in dataset]
    enc_dataset_array = np.array(encoded_dataset)
    return enc_dataset_array
  
  def decode_dataset(self, dataset):
    '''
    Reverse encode_dataset.
    args: dataset (list of sentences)
    returns: list of sentences
    '''
    return [self.decode_sentence(s) for s in dataset]
    
    
  def get_vocabulary(self):
    return self.vocab2num

from gensim import models
import tensorflow as tf
import numpy as np

def load_embedding(session, vocab, emb, path, dim_embedding, vocab_size):
    '''
      session        Tensorflow session object
      vocab          A dictionary mapping token strings to vocabulary IDs
      emb            Embedding tensor of shape vocabulary_size x dim_embedding
      path           Path to embedding file
      dim_embedding  Dimensionality of the external embedding.
    '''

    print("Loading external embeddings from %s" % path)

    model = models.KeyedVectors.load_word2vec_format(path, binary=False)  
    external_embedding = np.zeros(shape=(vocab_size, dim_embedding))
    matches = 0

    for tok, idx in vocab.items():
        if tok in model.vocab:
            external_embedding[idx] = model[tok]
            matches += 1
        else:
            print("%s not in embedding file" % tok)
            external_embedding[idx] = np.random.uniform(low=-0.25, high=0.25, size=dim_embedding)
        
    print("%d words out of %d could be loaded" % (matches, vocab_size))
    
    pretrained_embeddings = tf.placeholder(tf.float32, [None, None]) 
    assign_op = emb.assign(pretrained_embeddings)
    session.run(assign_op, {pretrained_embeddings: external_embedding}) # here, embeddings are actually set

# http://colah.github.io/posts/2015-08-Understanding-LSTMs/

class Network:
  
  def __init__(self,
               dataset, 
               state_size=512,
               batch_size=64,
               embed_size=100,
               vocab_size = 20000,
               pretrained_embeddings=False,
               sentence_size=30-1,
               use_projection=False,
               projection_size=512,
               generative=False
              ):
    
    
    # Determine how many batches are we needed
    self.dataset = dataset
    self.state_size = state_size
    self.batch_size = batch_size
    self.embed_size = embed_size
    self.sentence_size = sentence_size
    self.num_batches = math.ceil(num_sentences/batch_size)
    self.vocab_size = vocab_size
    self.pretrained_embeddings=pretrained_embeddings
    
    self.use_projection = use_projection
    self.projection_size = projection_size
    self.generative=generative
    
    tf.reset_default_graph()
    
    self.__define_graph()
    self.__define_generative_graph()
      
    
    
  def __define_graph(self):
    # tf.reset_default_graph()
    
    # ========================== Define Components =============================

    with tf.name_scope('input'):
      # Each word is represented as [0, 0, ... , 1, 0, 0, ...] with only one 1
      self.batchX_placeholder = tf.placeholder(tf.int32, [None, self.sentence_size], name='batchX_placeholder')
      self.batchY_placeholder = tf.placeholder(tf.int32, [None, self.sentence_size], name='batchY_placeholder')
      
      
    with tf.name_scope('embeddings'):
      # The embedding layer
      self.embedding = tf.Variable(tf.random_uniform((self.vocab_size, self.embed_size), -1, 1), name='embedding')

    if self.use_projection:
      
      with tf.name_scope('projection_layer'):
       # The dense layer that converts words to logits
        self.W_p = tf.Variable(np.random.rand(self.state_size, self.projection_size), dtype=tf.float32, name='projection_weights')
        self.b_p = tf.Variable(np.zeros((1,self.projection_size)), dtype=tf.float32, name='projection_bias') 

      with tf.name_scope('dense_layer'):
       # The dense layer that converts words to logits
        self.W_l = tf.Variable(np.random.rand(self.projection_size, self.vocab_size), dtype=tf.float32, name='dense_weights')
        self.b_l = tf.Variable(np.zeros((1,self.vocab_size)), dtype=tf.float32, name='dense_bias')

        
    else:
      
      with tf.name_scope('dense_layer'):
          # The dense layer that converts words to logits
          self.W_l = tf.Variable(np.random.rand(self.state_size, self.vocab_size), dtype=tf.float32, name='dense_weights')
          self.b_l = tf.Variable(np.zeros((1,self.vocab_size)), dtype=tf.float32, name='dense_bias')
    

    # ========================= Define Graph ===================================
    with tf.name_scope('main_lstm'):
      # Unpack columns
      self.inputs_series = tf.unstack(self.batchX_placeholder, axis=1)
      self.labels_series = tf.unstack(self.batchY_placeholder, axis=1)

      self.h_series = []
      self.logits_series = []
      self.masks = []

      self.lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self.state_size, 
                                          initializer=tf.contrib.layers.xavier_initializer(), state_is_tuple=True, name="main_cell")

      state = (tf.zeros((self.batch_size,self.state_size) , name="h_init"), 
               tf.zeros((self.batch_size,self.state_size), name="C_init"))

      for current_input, current_label in zip(self.inputs_series,self.labels_series):

          current_input = tf.reshape(current_input, [self.batch_size, 1])

          lookup = tf.nn.embedding_lookup(self.embedding, current_input)

          x = tf.reshape(lookup, [self.batch_size, self.embed_size])
          
          h_t, state = self.lstm_cell(x,state) 
          
          if self.use_projection:
            h_t_bar = tf.matmul(h_t, self.W_p) + self.b_p
            self.h_series.append(h_t_bar)
            
          else: 
            self.h_series.append(h_t)

          self.masks.append(tf.cast(tf.not_equal(current_label, PAD), tf.float32))


    with tf.name_scope("cross_entropy"):

      self.logits_series = [tf.matmul(h, self.W_l) + self.b_l for h in self.h_series]
      # self.logits_series = [tf.matmul(tf.matmul(h, self.W_p), self.W_l) + self.b_l for h in self.h_series]
      
      self.predictions_series = [tf.nn.softmax(logits) for logits in self.logits_series]
      
      self.losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels) \
                for logits, labels in zip(self.logits_series,self.labels_series)]

      self.sentences_len = tf.reduce_sum(tf.stack(self.masks,axis=1), axis=1)
      
      self.masked_sentences = [l*m for l,m in zip(self.losses,self.masks)]
      
      self.sum_per_sentence = tf.reduce_sum(self.masked_sentences, axis=0)
      
      self.reduced_losses = self.sum_per_sentence / self.sentences_len
      
      self.perplexity_per_sentence = tf.exp(self.reduced_losses)

      self.total_loss = tf.reduce_mean(self.reduced_losses)
      
      self.avg_perplexity = tf.reduce_mean(self.perplexity_per_sentence)

      self.optimizer = tf.train.AdamOptimizer()
      
      trainable_params = tf.trainable_variables()
      
      gradients = tf.gradients(self.total_loss, trainable_params, name="compute_gradients")

      clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0, name="clip_gradients")
      
      self.train_step = self.optimizer.apply_gradients(zip(clipped_gradients, trainable_params))
 
  
            

  def train(self, 
            num_epochs = 4, 
            path_to_save_model="/cluster/home/saioanni/NLU/models/mymodel.ckpt", 
            path_to_embedding="/cluster/home/saioanni/NLU/data/wordembeddings-dim100.word2vec"):
                                 
    with tf.Session() as sess:
        sess.run(tf.initialize_all_variables())
        
        if self.pretrained_embeddings:
          load_embedding(sess, encoder.get_vocabulary() , 
                         self.embedding,
                         path_to_embedding,
                         self.embed_size, 
                         self.vocab_size)
        
        
        saver = tf.train.Saver()
        loss_list = []

        for epoch_idx in range(num_epochs):
            print("New data, epoch", epoch_idx)
            start = time.time()
            for batch_idx in range(self.num_batches - 1):
                
                start_idx = batch_idx*self.batch_size
                end_idx = start_idx + self.batch_size

                batchX = self.dataset[start_idx:end_idx,:self.sentence_size]
                batchY = self.dataset[start_idx:end_idx,1:]

                _total_loss, _train_step,  _predictions_series = sess.run(
                    [self.total_loss, self.train_step, self.predictions_series],
                    feed_dict={
                        self.batchX_placeholder:batchX,
                        self.batchY_placeholder:batchY,                        
                    })

                loss_list.append(_total_loss)

            end = time.time()
            print("Step",batch_idx, "Loss", _total_loss)
            print("Time per epoch", end - start)

        saver.save(sess, path_to_save_model)
        

  def evaluate(self, evaluation_set, path_to_model="/cluster/home/saioanni/NLU/models/mymodel.ckpt"):
    
    num_of_batches = len(evaluation_set) //self.batch_size
    saver = tf.train.Saver()
    perps = []
    
    with tf.Session() as sess:
        # Restore variables from disk.
        saver.restore(sess, path_to_model)
        
        for batch_idx in range(num_of_batches - 1 ):
          
          start_idx = batch_idx*self.batch_size
          end_idx = start_idx + self.batch_size

          batchX = evaluation_set[start_idx:end_idx,:self.sentence_size]
          batchY = evaluation_set[start_idx:end_idx,1:]

          _avg_perplexity, _losses = sess.run(
              [self.avg_perplexity, self.sum_per_sentence],
              feed_dict={
                  self.batchX_placeholder:batchX,
                  self.batchY_placeholder:batchY,
              })
          
          perps.append(_avg_perplexity)
    
    return perps
  
  def test(self, evaluation_set, path_to_model="/cluster/home/saioanni/NLU/models/mymodel.ckpt"):
    
    num_of_batches = len(evaluation_set) //self.batch_size
    saver = tf.train.Saver()
    perps = []
    
    with tf.Session() as sess:
        # Restore variables from disk.
        saver.restore(sess, path_to_model)
        
        for batch_idx in range(num_of_batches - 1 ):
          
          start_idx = batch_idx*self.batch_size
          end_idx = start_idx + self.batch_size

          batchX = evaluation_set[start_idx:end_idx,:self.sentence_size]
          batchY = evaluation_set[start_idx:end_idx,1:]

          _avg_perplexity, _losses = sess.run(
              [self.avg_perplexity, self.sum_per_sentence],
              feed_dict={
                  self.batchX_placeholder:batchX,
                  self.batchY_placeholder:batchY,
              })
          
          perps.append(_avg_perplexity)
    
    return perps
  def __define_generative_graph(self):
    
  # ========================== Define Components =============================

    with tf.name_scope('input'):
      # Each word is represented as [0, 0, ... , 1, 0, 0, ...] with only one 1
      self.word = tf.placeholder(tf.int32, [self.batch_size, 1], name='word') 
      self.hidden_state_1 = tf.placeholder(tf.float32, [self.batch_size, self.state_size], name='hidden_state_1')
      self.hidden_state_2 = tf.placeholder(tf.float32, [self.batch_size, self.state_size], name='hidden_state_2')

    

    # ========================= Define Graph ===================================
    with tf.name_scope('main_lstm'):
    
      '''
      self.lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self.state_size, 
                                            initializer=tf.contrib.layers.xavier_initializer(),
                                            reuse = True,
                                            state_is_tuple=True, name="main_cell")
      '''

      lookup = tf.nn.embedding_lookup(self.embedding, self.word)

      x = tf.reshape(lookup, [self.batch_size, self.embed_size])

      h_t, self.next_state = self.lstm_cell(x, (self.hidden_state_1, self.hidden_state_2)) 

      h_t_bar = tf.matmul(h_t, self.W_p) + self.b_p

      logits = tf.matmul(h_t_bar, self.W_l) + self.b_l

      self.next_word = tf.argmax(logits, axis = 1)
    
  
  def continuations(self, continutation_set, path_to_model="/cluster/home/saioanni/NLU/models/mymodel.ckpt"):
    
    '''
    var_list = {'embeddings/embedding':self.embedding,
                'projection_layer/projection_weights': self.W_p,
                'projection_layer/projection_bias':self.b_p,
                'dense_layer/dense_weights': self.W_l,
                'dense_layer/dense_bias': self.b_l,
                'main_cell/kernel': self.lstm_cell
               }
    '''
    
    saver = tf.train.Saver()
    perps = []  
    sentences = []
    
    with tf.Session() as sess:
        # Restore variables from disk.
        saver.restore(sess, path_to_model)
        
        print(continuation_set.shape)
        for i in range(continutation_set.shape[0])[:100]:
          
          sentence = []
          
          _next_state_1 = np.zeros((1,self.state_size), dtype=float)
          _next_state_2 = np.zeros((1,self.state_size), dtype=float)

          
          for j in range(continuation_set.shape[1]):

            word = continuation_set[i:i+1,j:j+1]

            if np.any(word == PAD):
              word = _next_word
            
            word = np.reshape(word, (1,1))
            
            _next_word, _next_state = sess.run(
                [self.next_word, self.next_state],
                feed_dict={
                    self.word: word,
                    self.hidden_state_1: _next_state_1,
                    self.hidden_state_2: _next_state_2
                })
            
            if np.any(word == PAD):
              sentence.append(_next_word[0])
            else:
              sentence.append(word[0][0])
           
          sentences.append(sentence)
    print(len(sentences))  
    return sentences

# PREPROCESSING SETTINGS

num_sentences = 1000000  # Number of senteces to be read from the training file
max_length = 30 # Max length of the sentence to be kept. Notice that <bos> and eos should also be included
vocab_size = 20000 - 4 # Number of words in the vocabulary. Subtract by 4 to include special characters
extra_words = ['<bos>','<unk>','<pad>','<eos>'] # Special words


sentences = get_sentences_from_file(path=path,
                                    num_sentences=num_sentences,
                                    max_length=max_length)

vocabulary = build_vocabulary(sentences,
                              vocab_size=vocab_size,
                              extra_words=extra_words)

encoder = Encoder(vocabulary)
dataset = encoder.encode_dataset(sentences)
network = Network(dataset, state_size=1024, use_projection=True, pretrained_embeddings=True, projection_size=512, )
network.train(num_epochs=4)

# evaluation_sentences = get_sentences_from_file(path=eval_path)
# evaluation_set = encoder.encode_dataset(evaluation_sentences)
# preps = network.evaluate(evaluation_set)
# print(np.mean(preps))

test_sentences = get_sentences_from_file(path=test_path)
test_set = encoder.encode_dataset(test_sentences)

preps = network.test(test_set)
print(np.mean(preps))





# network = Network(dataset, state_size=1024, use_projection=True, projection_size=512, batch_size=1, generative=True)
# 
# continuation_sentences = get_sentences_from_file(path=cont_path, add_eos=False)
# 
# continuation_set = encoder.encode_dataset(continuation_sentences)
# print(type(continuation_set))
# 
# results = network.continuations(continuation_set)
# 
# encoder = Encoder(vocabulary)
# output_sentences = encoder.decode_dataset(results)
# [print(s) for s in output_sentences]

'''
from tensorflow.python import pywrap_tensorflow
import os

reader = pywrap_tensorflow.NewCheckpointReader('./drive/My Drive/Colab Notebooks/mymodel.ckpt')
var_to_shape_map = reader.get_variable_to_shape_map()

for key in var_to_shape_map:
    print("tensor_name: ", key)
    print(reader.get_tensor(key)) #

total_parameters = 0
for variable in tf.trainable_variables():
   # shape is an array of tf.Dimension
    shape = variable.get_shape()
    #print(shape)
    #print(len(shape))
    variable_parameters = 1
    for dim in shape:
        #print(dim)
        variable_parameters *= dim.value
    #print(variable_parameters)
    total_parameters += variable_parameters
print(total_parameters)

tf.all_variables()

saver = tf.train.Saver()
with tf.Session() as sess:
    # Restore variables from disk.
    saver.restore(sess, "./drive/My Drive/Colab Notebooks/mymodel.ckpt")
    _predictions_series = sess.run(
        predictions_series,
        feed_dict={
            batchX_placeholder:batchX,
            batchY_placeholder:batchY,
            h_init:_h_t,
            C_init:_C_t
        })

    # predicted_words = prob2words(_predictions_series,batch_size)
    print(prob2words(_predictions_series, batch_size))

print(_predictions_series)

a = np.array([[1,2,3],[4,5,6]])
np.argmax(a,axis=1)

probabilities = _predictions_series
real_sentences = batchY
# print(real_sentences)
sentences = []
number_sentences = []
for ws in probabilities:
    score = sum(ws[real_senteces])
    numbers= np.argmax(ws,axis=1).tolist()
    words = [num2vocab[n] for n in numbers]
    words = tuple(words)
    sentences.append(words)

# print(number_sentences)
number_sentences =list(zip(*number_sentences))
for r,p in zip(real_sentences,number_sentences):
    print(list(r))
    print(list(p))
    
sentences = list(zip(*sentences))
for s in sentences:
    print(list(s))

a = np.array(probabilities)
# a[:,batchY
real = batchY.T
# print(real)
score = np.log2(np.sum(a[:,:,real]))
print(score/(30*10))

!ls drive/'My Drive'/'Colab Notebooks'
'''

