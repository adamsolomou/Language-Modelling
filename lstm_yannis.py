# -*- coding: utf-8 -*-
"""lstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14jqDQ4AH0yCcqaMijD6_eLOp8euHPU52
"""

from __future__ import print_function, division
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from collections import Counter
from google.colab import drive
import time
drive.mount('/content/drive')

device_name = tf.test.gpu_device_name()

if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
  
print('Found GPU at: {}'.format(device_name))

path = "./drive/My Drive/Colab Notebooks/sentences.train"


# we need these initilizations here 
BOS = 0
UNK = 1
PAD = 2
EOS = 3

def prepare_dataset(path=path, 
                    num_sentences=2000000,
                    max_length=30,
                    vocab_size=20000-4,
                    extra_words=['<bos>','unk','<pad>','<eos>']):
  
  '''
  Description:
  Read sentences from file. Read the number of sentences indicated by num_sentences. 
  Each sentence is represented by a list of words.
  The sentences are initialized as a list which consists of <pad> chars.
  If read lines returns '' then there are no more sentences to read, thus we should
  stop reading the file.
  Sentences that exceed max_length do not count.
  Otherwise we set sentence[0] as <bos> and add the words and increase their corresponding counter
  After having read all the words in the sentence we add '<eos>'.
  Then select the most common words and create a set of them which then we unite with the 
  set of the extra/special words.
  Finally create a two way hash dictionary.
  Args: the name of the file to read
  Returns: a tuple of list of sentences and vocabulary
  
  '''


  file = open(path,'r')

  num_lines = num_sentences
  lines = []

  vocabulary = Counter()
  sentences = [] # Each sentecse is rep

  row = 0
  while num_lines > 0:
      sentence = max_length*['<pad>']
      line = file.readline().strip()

      if line == '': break

      words = line.split()
      sentence_length = len(words)

      if len(words) > max_length - 2:
          continue

      sentence[0] = '<bos>'

      for i,w in enumerate(words):
          sentence[i+1] = w
          vocabulary[w] += 1

      sentence[sentence_length] = '<eos>'

      num_lines -= 1
      sentences.append(sentence)

  vocabulary,_ = zip(*list(vocabulary.most_common(vocab_size - len(extra_words))))
  vocabulary = extra_words + list(vocabulary)
  assert(vocabulary[PAD] == '<pad>')
  return (sentences, vocabulary)

class Encoder:
  
  def __init__(self, vocabulary):
    
    self.vocabulary = vocabulary
    self.vocab2num = dict([(w,i) for i,w in enumerate(self.vocabulary)])
    self.num2vocab = dict([(i,w) for i,w in enumerate(self.vocabulary)])
    assert(self.num2vocab[self.vocab2num['<pad>']] == '<pad>')
  
  def encode_word(self, word):
    return self.vocab2num.get(word,'<unk>')
  
  def decode_word(self, word):
    return self.num2vocab[word]
    
  def encode_sentence(self, sentence):
    '''
    Returns the encoded sentence; Each word is converted to number.
    '''
    return [self.vocab2num.get(w,'<unk>') for w in sentence]
  
  def decode_sentence(self, sentence):
    '''
    Returns the decoded sentence; The reversed procedure than the above.
    Notice that the reverse procedure may not be able to restore the sentence
    completely since <unk> corresponds to more than one word
    '''
    return [self.num2vocab[n] for n in sentence]
  
  def encode_dataset(self, dataset):
    '''
    Encode the whole data set.
    args: dataset (list of sentences)
    returns: np.array of numbers 
    '''
    encoded_dataset = [self.encode_sentence(s) for s in dataset]
    enc_dataset_array = np.array(encoded_dataset)
    return enc_dataset_array
  
  def decode_dataset(self, dataset):
    '''
    Reverse encode_dataset.
    args: dataset (list of sentences)
    returns: list of sentences
    '''
    return dataset.tolist()

# http://colah.github.io/posts/2015-08-Understanding-LSTMs/

class Network:
  
  def __init__(self,
               dataset, 
               num_epochs=6,
               state_size=6,
               batch_size=64,
               embed_size=100,
               vocab_size = 20000,
               sentence_size=30-1
              ):
    
    
    # Determine how many batches are we needed
    self.dataset = dataset
    self.num_epochs = num_epochs
    self.state_size = state_size
    self.batch_size = batch_size
    self.embed_size = embed_size
    self.sentence_size = sentence_size
    self.num_batches = num_sentences//batch_size
    self.vocab_size = vocab_size
    self.__define_graph()
    
  def __define_graph(self):
    # tf.reset_default_graph()
    
    # ========================== Define Components =============================

    # Each word is represented as [0, 0, ... , 1, 0, 0, ...] with only one 1
    self.batchX_placeholder = tf.placeholder(tf.int32, [self.batch_size, self.sentence_size], name='batchX_placeholder')
    self.batchY_placeholder = tf.placeholder(tf.int32, [self.batch_size, self.sentence_size], name='batchY_placeholder')

    # The embedding layer
    self.embedding = tf.Variable(tf.random_uniform((self.vocab_size, self.embed_size), -1, 1))

    # The dense layer that converts words to logits
    self.W_l = tf.Variable(np.random.rand(self.state_size, self.vocab_size), dtype=tf.float32)
    self.b_l = tf.Variable(np.zeros((1,self.vocab_size)), dtype=tf.float32)

    # ========================= Define Graph ===================================
    # Unpack columns
    self.inputs_series = tf.unstack(self.batchX_placeholder, axis=1)
    self.labels_series = tf.unstack(self.batchY_placeholder, axis=1)

    self.h_series = []
    self.logits_series = []
    self.masks = []
    
    self.lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self.state_size, 
                                        initializer=tf.contrib.layers.xavier_initializer(), state_is_tuple=True)

    state = (tf.zeros((self.batch_size,self.state_size) , name="h_init"), 
             tf.zeros((self.batch_size,self.state_size), name="C_init"))

    for current_input, current_label in zip(self.inputs_series,self.labels_series):
      
        current_input = tf.reshape(current_input, [self.batch_size, 1])
        
        lookup = tf.nn.embedding_lookup(self.embedding, current_input)
        
        x = tf.reshape(lookup, [self.batch_size, self.embed_size])
        h_t, state = self.lstm_cell(x,state)
        self.h_series.append(h_t)

        self.masks.append(tf.cast(tf.not_equal(current_label, PAD), tf.float32))



    self.logits_series = [tf.matmul(h, self.W_l) + self.b_l for h in self.h_series]
    
    self.predictions_series = [tf.nn.softmax(logits) for logits in self.logits_series]

    # print(self.predictions_series[0])
    
    self.losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels) \
              for logits, labels in zip(self.logits_series,self.labels_series)]

    # print(self.losses)
    
    self.sentences_len = tf.reduce_sum(tf.stack(self.masks,axis=1), axis=1)
    
    # print(self.sentences_len)

    self.reduced_losses = [l*m for l,m in zip(self.losses,self.masks)] / self.sentences_len
    
    # print(self.reduced_losses)

    self.total_loss = tf.reduce_mean(self.losses)
    
    # print(self.total_loss)
    
    self.train_step = tf.train.AdagradOptimizer(0.3).minimize(self.total_loss)
 
  def train(self, path_to_save_model="./drive/My Drive/Colab Notebooks/mymodel.ckpt"):
                                 
    with tf.Session() as sess:
        sess.run(tf.initialize_all_variables())
        saver = tf.train.Saver()
        loss_list = []

        for epoch_idx in range(self.num_epochs):
            print("New data, epoch", epoch_idx)
            start = time.time()
            print(self.num_batches)
            for batch_idx in range(self.num_batches - 1):
                
                start_idx = batch_idx*self.batch_size
                end_idx = start_idx + self.batch_size

                batchX = self.dataset[start_idx:end_idx,:30-1]
                batchY = self.dataset[start_idx:end_idx,1:]

                _total_loss, _train_step,  _predictions_series = sess.run(
                    [self.total_loss, self.train_step, self.predictions_series],
                    feed_dict={
                        self.batchX_placeholder:batchX,
                        self.batchY_placeholder:batchY,                        
                    })

                loss_list.append(_total_loss)

            end = time.time()
            print("Step",batch_idx, "Loss", _total_loss)
            print(end - start)

        saver.save(sess, path_to_save_model)

# PREPROCESSING SETTINGS

num_sentences = (2000//64)*64 # Number of senteces to be read from the training file
max_length = 30 # Max length of the sentence to be kept. Notice that <bos> and eos should also be included
vocab_size = 20000 - 4 # Number of words in the vocabulary. Subtract by 4 to include special characters
extra_words = ['<bos>','<unk>','<pad>','<eos>'] # Special words



(sentences,vocabulary) = prepare_dataset(path=path, 
                                         num_sentences=num_sentences, 
                                         max_length=max_length,
                                         vocab_size=vocab_size,
                                         extra_words=extra_words)

encoder = Encoder(vocabulary)
dataset = encoder.encode_dataset(sentences)
network = Network(dataset)
network.train()

import time

def prob2words(probabilites,batch_size):
    for b in range(batch_size):
        numbers_df = pd.DataFrame(np.argmax(probabilites[b],axis=1))
        words = numbers_df.apply(lambda x: num2vocab[x]).values
        return words

saver = tf.train.Saver()
with tf.Session() as sess:
    # Restore variables from disk.
    saver.restore(sess, "./drive/My Drive/Colab Notebooks/mymodel.ckpt")
    _predictions_series = sess.run(
        predictions_series,
        feed_dict={
            batchX_placeholder:batchX,
            batchY_placeholder:batchY,
            h_init:_h_t,
            C_init:_C_t
        })

    # predicted_words = prob2words(_predictions_series,batch_size)
    print(prob2words(_predictions_series, batch_size))

print(_predictions_series)

a = np.array([[1,2,3],[4,5,6]])
np.argmax(a,axis=1)

probabilities = _predictions_series
real_sentences = batchY
# print(real_sentences)
sentences = []
number_sentences = []
for ws in probabilities:
    score = sum(ws[real_senteces])
    numbers= np.argmax(ws,axis=1).tolist()
    words = [num2vocab[n] for n in numbers]
    words = tuple(words)
    sentences.append(words)

# print(number_sentences)
number_sentences =list(zip(*number_sentences))
for r,p in zip(real_sentences,number_sentences):
    print(list(r))
    print(list(p))
    
sentences = list(zip(*sentences))
for s in sentences:
    print(list(s))

a = np.array(probabilities)
# a[:,batchY
real = batchY.T
# print(real)
score = np.log2(np.sum(a[:,:,real]))
print(score/(30*10))

!ls drive/'My Drive'/'Colab Notebooks'

